{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer o download do dataset IMDB\n",
    "# Este dataset está disponível nos Keras datasets: https://keras.io/api/datasets/imdb/\n",
    "# A informação está completamente processada e pronta a usar pelo classificador\n",
    "\n",
    "# O Dataset em bruto pode ser obtido aqui: https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# Neste último caso seria necessário aplicar pré-processamento de texto antes de criar o classificador\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Só vai considerar um vocabulário com esta dimensão\n",
    "# As palavras são ordenadas pela frequência no conjunto de treino e apenas estas são mantidas\n",
    "max_features = 10000\n",
    "\n",
    "# Obter o dataset, dividindo-o \n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar a dimensão dos conjuntos de treino e de teste\n",
    "\n",
    "print('Train: ', x_train.shape)\n",
    "print('Test: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar algumas reviews. Altere o valor da variável review para aceder a outro texto\n",
    "# O dataset já vem tratado e cada exemplo é um array numpy de inteiros, em que cada inteiro representa uma palavra\n",
    "# A pontuação já foi removida, as palavras foram separadas por espaços e todos os caracteres foram convertidos para minúscula\n",
    "# Os inteiros estão organizados por frequência, logo os menores inteiros correspondem a palavras mais frequentes\n",
    "# Os inteiros 0, 1 e 2 tem um significado especial: padding, sos (starting of sequence), unk (unknown)\n",
    "\n",
    "review = 0\n",
    "\n",
    "print(\"Numero de palavras: \" ,len(x_train[review]))\n",
    "print(x_train[review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descodificar os valores e consultar a review\n",
    "# O método get_word_index() obtém um dicionário que mapeia as palavras para o seu valor numérico\n",
    "# Labels: 0(Bad), 1(Good)\n",
    "\n",
    "review = 0\n",
    "tam = len(x_train[review])\n",
    "\n",
    "print('Label ', y_train[review])\n",
    "\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in x_train[review][:tam]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cortar as reviews para um determinado tamanho máximo: neste caso, o tamanho máximo será 20\n",
    "# Por omissão, o corte é feito no início \n",
    "# O classificador terá que efetuar previsões tendo em consideração apenas as ultimas 20 palavras da review\n",
    "#https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "\n",
    "maxlen = 20\n",
    "\n",
    "x_trainP = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_testP = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar algumas reviews\n",
    "# O sentido positivo/negativo da review é evidente neste excerto de texto?\n",
    "\n",
    "review = 0\n",
    "tam = len(x_trainP[review])\n",
    "\n",
    "print('Label ', y_train[review])\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in x_trainP[review][:tam]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma rede baseline que processe os inputs de forma simples\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model_1 = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[maxlen, 1]),\n",
    "    keras.layers.Dense(10),\n",
    "    keras.layers.Dense(10),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_1.fit(x_trainP, y_train,\n",
    "                      epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o desempenho no conjunto de teste\n",
    "\n",
    "model_1.evaluate(x_testP, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar uma camada para embedding. O treino do embedding será feito pelo classificador\n",
    "# https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# A saída recebe as contribuições pesadas \n",
    "\n",
    "model_2 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(10000, 8, input_length= maxlen),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10),\n",
    "    keras.layers.Dense(10),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_2.fit(x_trainP, y_train,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o desempenho no conjunto de teste\n",
    "\n",
    "model_2.evaluate(x_testP, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir os nós da rede neuronal das camadas escondidas por células LSTM\n",
    "\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# A saída recebe as contribuições pesadas \n",
    "\n",
    "model_3 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(10000, 8, input_length= maxlen),\n",
    "    keras.layers.LSTM(10, return_sequences=True),\n",
    "    keras.layers.LSTM(10),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_3.fit(x_trainP, y_train,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o desempenho no conjunto de teste\n",
    "\n",
    "model_3.evaluate(x_testP, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizar um embedding pré-treinado\n",
    "# Neste exemplo será usado o embedding GloVe: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "# Fazer o download do embedding\n",
    "# Depois disso guardar na diretoria indicada \n",
    "\n",
    "glove_dir = 'glove.6B/'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma matriz onde será colocado o embedding\n",
    "# Tem max_words linhas e 100 colunas (a dimensão do embedding que vamos usar)\n",
    "\n",
    "embedding_dim = 100\n",
    "max_words = max_features\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim)) # matriz com zeros\n",
    "\n",
    "# Preencher a matriz com os dados do embedding pré-treinado\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um modelo igual ao anterior, alterando apenas a dimensão do embedding\n",
    "\n",
    "model_4 = keras.models.Sequential([\n",
    "    keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n",
    "    keras.layers.LSTM(10, return_sequences=True),\n",
    "    keras.layers.LSTM(10),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocar os valores do embedding pré-treinado na camada de embedding\n",
    "# Congelar estes pesos para que não se alterem durante o treino\n",
    "\n",
    "model_4.layers[0].set_weights([embedding_matrix])\n",
    "model_4.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model_4.fit(x_trainP, y_train,\n",
    "                      epochs=10,\n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o desempenho no conjunto de teste\n",
    "\n",
    "model_4.evaluate(x_testP, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
